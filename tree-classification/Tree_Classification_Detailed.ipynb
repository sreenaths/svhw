{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c330b39c",
   "metadata": {},
   "source": [
    "# <span style='color:green; font-family:Helvetica'> Classifying Pepper and Weeping Willow Trees </span>\n",
    "### <span style='color:green; font-family:Helvetica'> by DeepSquad </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec12503",
   "metadata": {},
   "source": [
    "<img src=\"images_trees/ICON.png\" alt=\"Image\" style=\"width:500px;height:400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d0d80",
   "metadata": {},
   "source": [
    "## <span style='color:green; font-family:Helvetica'> Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c09ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms #provides various image transformation functions\n",
    "from torchvision.datasets import ImageFolder #class allows us to load images from folders organized by class labels\n",
    "\n",
    "# Define the transformations to apply to the images\n",
    "# Compose: his function is used to create a sequence of transformations that will be applied to the images. \n",
    "# It takes a list of transformation functions as arguments and applies them in sequence.\n",
    "# Resize: Many pre-trained models, such as ResNet, VGG, and MobileNet, expect input images of this size\n",
    "# ToTensor: This transformation converts the images from the PIL Image format to a PyTorch tensor. \n",
    "# Tensors are the fundamental data structure used in PyTorch for efficient computation.\n",
    "# Normalize: normalizes the image tensor -Normalizing the input data to have zero mean and unit variance \n",
    "# Helps ensure that the model receives consistent and well-scaled inputs. \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the training dataset\n",
    "train_set = ImageFolder('images_trees/train', transform=transform)\n",
    "\n",
    "# Load the test dataset\n",
    "test_set = ImageFolder('images_trees/test', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b0405",
   "metadata": {},
   "source": [
    "## <span style='color:green; font-family:Helvetica'> Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #provides various functionalities for deep learning and neural networks\n",
    "import torchvision.models as models #for popular pre-trained models for computer vision tasks.\n",
    "\n",
    "# Load a pre-trained model ResNet-50 that has been pre-trained on the ImageNet dataset\n",
    "model = models.resnet50(pretrained=True) \n",
    "\n",
    "# Modify the last fully connected layer to match the number of classes\n",
    "# This line specifies the number of classes in your classification problem. \n",
    "# In this case, you have two classes: pepper tree and willow tree\n",
    "\n",
    "num_classes = 2\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# modifies the last fully connected layer of the ResNet-50 model to match the number of classes in your classification problem\n",
    "# torch.nn.Linear - used to replace the existing fully connected layer with a new one that has the desired number of output features\n",
    "# model.fc.in_features - retrieves the input size of the existing fully connected layer\n",
    "# num_classes - specifies the desired output size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173d719b",
   "metadata": {},
   "source": [
    "## <span style='color:green; font-family:Helvetica'> Model Fine-Tuning<span style='color:green; font-family:Helvetica'> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In deep learning models, the requires_grad attribute of a parameter determines whether gradients should be computed and stored for that parameter during the backward pass of training. \n",
    "#By default, this attribute is set to True, indicating that the parameter participates in the gradient computation.\n",
    "\n",
    "\n",
    "# Set requires_grad to False for all parameters except the last layer\n",
    "# This step is important because we want to freeze the weights of the pre-trained layers and avoid updating them during the fine-tuning process.\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set requires_grad to True for the parameters of the last layer\n",
    "# we allow gradients to be computed for these parameters during the backward pass. \n",
    "# This step is necessary because we want to train the parameters of the last layer from scratch or fine-tune them to fit the specific classification task.\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a661e",
   "metadata": {},
   "source": [
    "## <span style='color:green; font-family:Helvetica'> Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim #provides various optimization algorithms\n",
    "import torch.nn as nn #contains neural network related functions and classes\n",
    "from torch.utils.data import DataLoader #provides utilities for data loading and manipulation\n",
    "\n",
    "# Create data loaders for training and testing - to efficiently load and process the training and testing datasets in batches during the training and evaluation phases\n",
    "# Loading data in batches helps in utilizing GPU parallelism and reduces memory requirements.\n",
    "# shuffle=True argument for train_loader ensures that the training data is randomly shuffled before each epoch, which introduces randomness and prevents the model from memorizing the order of the samples.\n",
    "# shuffle=False keeps the testing data in the original order during evaluation to ensure consistency and allow comparison with the ground truth labels.\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffl\n",
    "                         e=False)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# The loss function (nn.CrossEntropyLoss()) - to calculate the discrepancy between the predicted outputs and the ground truth labels during training.\n",
    "# It combines softmax activation and the negative log-likelihood loss.\n",
    "# optimizer (optim.SGD()) - implements the stochastic gradient descent algorithm and updates the model's parameters based on the computed gradients\n",
    "# model.parameters() provides the parameters of the model that need to be optimized.\n",
    "# lr=0.001 sets the learning rate to 0.001,  determines the step size for parameter updates during optimization.\n",
    "# momentum=0.9 sets the momentum to 0.9,  helps accelerate the optimization process by incorporating information from previous parameter updates.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 10 defines the number of times the model will iterate over the entire training dataset.\n",
    "# device is determined based on the availability of a CUDA-enabled GPU. It ensures that the model and data are moved to the appropriate device (GPU or CPU) for computation.\n",
    "# model.to(device) moves the model's parameters to the specified device.\n",
    "\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# The training loop iterates over the specified number of epochs and performs the below steps\n",
    "# Sets the model in the training mode (model.train()) to enable gradient computation and parameter updates.\n",
    "# Initializes the running_loss variable to keep track of the cumulative loss for each epoch.\n",
    "# For each batch of images and labels from the training data --\n",
    "# Moves the data to the specified device (device) for computation.\n",
    "# Clears the gradients accumulated in the optimizer (optimizer.zero_grad()).\n",
    "# Forward passes the images through the model to obtain the predicted outputs.\n",
    "# Calculates the loss between the predicted outputs and the ground truth labels using the specified loss function.\n",
    "# Backpropagates the gradients through the model and updates the model's parameters using the optimizer.\n",
    "# Accumulates the batch loss (loss.item()) to the running_loss.\n",
    "# Prints the average loss per batch for the current epoch.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f001c7",
   "metadata": {},
   "source": [
    "## <span style='color:green; font-family:Helvetica'> Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a function evaluate that is used to evaluate the performance of a trained model on a given data loader\n",
    "# takes two arguments: model (the trained model) and data_loader (the data loader containing the test dataset).\n",
    "# model.eval() sets the model to evaluation mode, which disables the gradient calculations and other operations specific to training.\n",
    "# correct and total are initialized to keep track of the number of correctly classified samples and the total number of samples, respectively.\n",
    "# torch.no_grad() is a context manager that disables gradient calculations and reduces memory consumption during the evaluation loop.\n",
    "# The loop iterates over the batches of images and labels from the data_loader.\n",
    "# images and labels are moved to the device (GPU) if available.\n",
    "# outputs contains the predicted class probabilities for the images obtained from the model.\n",
    "# torch.max(outputs.data, 1) finds the maximum value and its corresponding index along the dimension 1, which represents the predicted class labels.\n",
    "# total is updated by adding the size of the current batch of labels.\n",
    "# correct is updated by summing the number of correctly predicted labels.\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "# accuracy is calculated by dividing the number of correctly predicted labels (correct) by the total number of labels (total) and multiplying by 100 to obtain a percentage value.\n",
    "# The evaluate function is called with the trained model and the test_loader.\n",
    "# The returned accuracy value is stored in test_accuracy.\n",
    "test_accuracy = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2271fcd",
   "metadata": {},
   "source": [
    "## <span style='color:green; font-family:Helvetica'> Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save() is a function provided by PyTorch that allows us to save the state dictionary of an object to a file.\n",
    "# model.state_dict() returns a dictionary containing the parameters and persistent buffers of the model\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pt')\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b77d6e",
   "metadata": {},
   "source": [
    "## <span style='color:green; font-family:Helvetica'> Streamlit UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad826dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run on command line\n",
    "#streamlit run tree_classification_server.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
